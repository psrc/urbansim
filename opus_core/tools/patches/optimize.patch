Common subdirectories: optimize_old//benchmarks and optimize_para//benchmarks
diff -Nu optimize_old//__init__.py optimize_para//__init__.py
--- optimize_old//__init__.py	2012-06-22 19:21:18.699016376 -0700
+++ optimize_para//__init__.py	2012-06-28 13:20:36.110691606 -0700
@@ -143,6 +143,7 @@
 from minpack import *
 from zeros import *
 from anneal import *
+from psa import panneal
 from lbfgsb import fmin_l_bfgs_b
 from tnc import fmin_tnc
 from cobyla import fmin_cobyla
diff -Nu optimize_old//optimize.py optimize_para//optimize.py
--- optimize_old//optimize.py	2012-06-22 19:21:18.699016376 -0700
+++ optimize_para//optimize.py	2012-06-28 13:20:10.070692462 -0700
@@ -18,17 +18,21 @@
 __all__ = ['fmin', 'fmin_powell', 'fmin_bfgs', 'fmin_ncg', 'fmin_cg',
            'fminbound', 'brent', 'golden', 'bracket', 'rosen', 'rosen_der',
            'rosen_hess', 'rosen_hess_prod', 'brute', 'approx_fprime',
-           'line_search', 'check_grad']
+           'line_search', 'check_grad','set_parallel','is_parallelizable']
 
 __docformat__ = "restructuredtext en"
+_parallelized_approx_gradient = [False]
+is_parallelizable = True
 
+import sys
 import numpy
 from numpy import atleast_1d, eye, mgrid, argmin, zeros, shape, \
      squeeze, vectorize, asarray, absolute, sqrt, Inf, asfarray, isinf
 from linesearch import \
      line_search_BFGS, line_search_wolfe1, line_search_wolfe2, \
      line_search_wolfe2 as line_search
-
+import threading, Queue
+from copy import copy
 
 # standard status messages of optimizers
 _status_message = {'success': 'Optimization terminated successfully.',
@@ -493,71 +497,111 @@
     else:
         return x
 
+def set_parallel(val):
+    _parallelized_approx_gradient[0] = val
 
-def approx_fprime(xk, f, epsilon, *args):
-    """Finite-difference approximation of the gradient of a scalar function.
-
-    Parameters
-    ----------
-    xk : array_like
-        The coordinate vector at which to determine the gradient of `f`.
-    f : callable
-        The function of which to determine the gradient (partial derivatives).
-        Should take `xk` as first argument, other arguments to `f` can be
-        supplied in ``*args``.  Should return a scalar, the value of the
-        gradient at `xk`.
-    epsilon : array_like
-        Increment to `xk` to use for determining the function gradient.
-        If a scalar, uses the same finite difference delta for all partial
-        derivatives.  If an array, should contain one value per element of
-        `xk`.
-    \*args : args, optional
-        Any other arguments that are to be passed to `f`.
-
-    Returns
-    -------
-    grad : ndarray
-        The partial derivatives of `f` to `xk`.
-
-    See Also
-    --------
-    check_grad : Check correctness of gradient function against approx_fprime.
-
-    Notes
-    -----
-    The function gradient is determined by the forward finite difference
-    formula::
-
-                 f(xk[i] + epsilon[i]) - f(xk[i])
-        f'[i] = ---------------------------------
-                            epsilon[i]
+def approx_fprime(xk,f,epsilon,*args):
+    if _parallelized_approx_gradient[0]==True:
+        return approx_fprime_parallel(xk,f,epsilon,*args)
+    else:
+        return approx_fprime_sequential(xk,f,epsilon,*args)
 
-    The main use of `approx_fprime` is in scalar function optimizers like
-    `fmin_bfgs`, to determine numerically the Jacobian of a function.
+def approx_fprime_sequential(xk,f,epsilon,*args):
+    f0 = f(*((xk,)+args))
+    grad = numpy.zeros((len(xk),), float)
+    ei = numpy.zeros((len(xk),), float)
+    for k in range(len(xk)):
+        ei[k] = epsilon
+        grad[k] = (f(*((xk+ei,)+args)) - f0)/epsilon
+        ei[k] = 0.0
+    return grad
 
-    Examples
-    --------
-    >>> from scipy import optimize
-    >>> def func(x, c0, c1):
-    ...     "Coordinate vector `x` should be an array of size two."
-    ...     return c0 * x[0]**2 + c1*x[1]**2
-
-    >>> x = np.ones(2)
-    >>> c0, c1 = (1, 200)
-    >>> eps = np.sqrt(np.finfo(np.float).eps)
-    >>> optimize.approx_fprime(x, func, [eps, np.sqrt(200) * eps], c0, c1)
-    array([   2.        ,  400.00004198])
+class WorkerThread(threading.Thread):
+    """ A worker thread that takes directory names from a queue, finds all
+        files in them recursively and reports the result.
+
+        Input is done by placing directory names (as strings) into the
+        Queue passed in dir_q.
+
+        Output is done by placing tuples into the Queue passed in result_q.
+        Each tuple is (thread name, dirname, [list of files]).
+
+        Ask the thread to stop by calling its join() method.
+    """
+    def __init__(self, f, input_q, result_q):
+        super(WorkerThread, self).__init__()
+        self.f = f
+        self.input_q = input_q
+        self.result_q = result_q
+        self.stoprequest = threading.Event()
+        self.daemon = True # when the main thread receives the KeyboardInterrupt, if it doesn't catch it or catches it but decided to terminate anyway, the whole process will terminate
+
+    def run(self):
+        # As long as we weren't asked to stop, try to take new tasks from the
+        # queue. The tasks are taken with a blocking 'get', so no CPU
+        # cycles are wasted while waiting.
+        # Also, 'get' is given a timeout, so stoprequest is always checked,
+        # even if there's nothing in the queue.
+        while not self.stoprequest.isSet():
+            try:
+                inp = self.input_q.get(True, 0.05)
+                try:
+                    res = self.f(*inp[1])
+                    self.result_q.put([inp[0],res])
+                except:
+                    print '----- ERROR ----- :', str(sys.exc_info()[0]).split('exceptions.')[1].replace("'>","")
+                    sys.exit(0)
+            except Queue.Empty:
+                continue
+
+    def join(self, timeout=None):
+        self.stoprequest.set()
+        super(WorkerThread, self).join(timeout)
+    
+def approx_fprime_parallel(xk,f,epsilon,*args):
+    import multiprocessing
+    nb_cores = multiprocessing.cpu_count() # detects the number of cores in the computer
 
-    """
-    f0 = f(*((xk,) + args))
-    grad = numpy.zeros((len(xk),), float)
     ei = numpy.zeros((len(xk),), float)
+    fi = numpy.zeros((len(xk),), float)
+    grad = numpy.zeros((len(xk),), float)
+    
+    # Create a single input and a single output queue for all threads.
+    input_q = Queue.Queue()
+    result_q = Queue.Queue()
+    # Create the "thread pool"
+    pool = [WorkerThread(f=f, input_q=input_q, result_q=result_q) for i in range(nb_cores)]
+    # Start all threads
+    for thread in pool:
+        thread.start()
+
+    # Give the workers some work to do
+    work_count = 0
+    f0 = None
+    input_q.put([-1,copy((xk,)+args)])
     for k in range(len(xk)):
-        ei[k] = 1.0
-        d = epsilon * ei
-        grad[k] = (f(*((xk+d,)+args)) - f0) / d[k]
+        work_count += 1
+        ei[k] = epsilon
+        input_q.put([k,copy(((xk+ei,)+args))])
         ei[k] = 0.0
 
+    # Now get all the results
+    while work_count > 0:
+        # Blocking 'get' from a Queue.
+        result = result_q.get()
+        if result[0] == -1:
+            f0 = result[1]
+        else:
+            fi[result[0]] = result[1]
+            work_count -= 1
+
+    # Computing the gradient
+    grad = (fi - f0)/epsilon
+        
+    # Ask threads to die and wait for them to do it
+    for thread in pool:
+        thread.join()
+    
     return grad
 
 def check_grad(func, grad, x0, *args):
diff -Nu optimize_old//psa.py optimize_para//psa.py
--- optimize_old//psa.py	1969-12-31 16:00:00.000000000 -0800
+++ optimize_para//psa.py	2012-06-28 13:20:28.534691855 -0700
@@ -0,0 +1,582 @@
+# Original Author: Anthony Tschirhard 2012 -- UC BERKELEY
+# Bug-fixes in 2012
+
+import pdb
+# pdb.set_trace()
+
+import numpy
+from numpy import asarray, tan, exp, ones, squeeze, sign, \
+     all, log, sqrt, pi, shape, array, minimum, where
+from numpy import random
+from copy import copy
+from multiprocessing import Process, Manager
+from threading import Thread
+import time
+
+numpy.random.seed(seed=1)
+
+__all__ = ['panneal']
+# parallel simulated anneal
+
+_double_min = numpy.finfo(float).min
+_double_max = numpy.finfo(float).max
+class base_schedule(object):
+    def __init__(self):
+        self.dwell = 20
+        self.learn_rate = 0.5
+        self.lower = -10
+        self.upper = 10
+        self.Ninit = 50
+        self.accepted = 0
+        self.tests = 0
+        self.feval = 0
+        self.iters = 0
+        self.k = 0
+        self.T = None
+
+    def init(self, **options):
+        self.__dict__.update(options)
+        self.lower = asarray(self.lower)
+        self.lower = where(self.lower == numpy.NINF, -_double_max, self.lower)
+        self.upper = asarray(self.upper)
+        self.upper = where(self.upper == numpy.PINF, _double_max, self.upper)
+        self.k = 0
+        self.accepted = 0
+        self.feval = 0
+        self.tests = 0
+        
+    def getstart_temp(self, best_state):
+        """ Find a matching starting temperature and starting parameters vector
+        i.e. find x0 such that func(x0) = T0.
+
+        Parameters
+        ----------
+        best_state : _state
+            A _state object to store the function value and x0 found.
+
+        Returns
+        -------
+        x0 : array
+            The starting parameters vector.
+        """
+
+        assert(not self.dims is None)
+        lrange = self.lower
+        urange = self.upper
+        fmax = _double_min
+        fmin = _double_max
+        for _ in range(self.Ninit):
+            x0 = random.uniform(size=self.dims)*(urange-lrange) + lrange
+            fval = self.func(x0, *self.args)
+            self.feval += 1
+            if fval > fmax:
+                fmax = fval
+            if fval < fmin:
+                fmin = fval
+                best_state.cost = fval
+                best_state.x = array(x0)
+
+        self.T0 = (fmax-fmin)*1.5
+        return best_state.x
+
+    def accept_test(self, dE):
+        T = self.T
+        self.tests += 1
+        if dE < 0:
+            self.accepted += 1
+            return 1
+        p = exp(-dE*1.0/self.boltzmann/T)
+        if (p > random.uniform(0.0, 1.0)):
+            self.accepted += 1
+            return 1
+        return 0
+
+    def update_guess(self, x0):
+        pass
+
+    def update_temp(self, x0):
+        pass
+
+
+#  A schedule due to Lester Ingber
+class fast_sa(base_schedule):
+    def init(self, **options):
+        self.__dict__.update(options)
+        if self.m is None:
+            self.m = 1.0
+        if self.n is None:
+            self.n = 1.0
+        self.c = self.m * exp(-self.n * self.quench)
+
+    def update_guess(self, x0):
+        x0 = asarray(x0)
+        u = squeeze(random.uniform(0.0, 1.0, size=self.dims))
+        T = self.T
+        y = sign(u-0.5)*T*((1+1.0/T)**abs(2*u-1)-1.0)
+        xc = y*(self.upper - self.lower)
+        xnew = x0 + xc
+        return xnew
+
+    def update_temp(self):
+        self.T = self.T0*exp(-self.c * self.k**(self.quench))
+        self.k += 1
+        return
+
+class cauchy_sa(base_schedule):
+    def update_guess(self, x0):
+        x0 = asarray(x0)
+        numbers = squeeze(random.uniform(-pi/2, pi/2, size=self.dims))
+        xc = self.learn_rate * self.T * tan(numbers)
+        xnew = x0 + xc
+        return xnew
+
+    def update_temp(self):
+        self.T = self.T0/(1+self.k)
+        self.k += 1
+        return
+
+class boltzmann_sa(base_schedule):
+    def update_guess(self, x0):
+        std = minimum(sqrt(self.T)*ones(self.dims), (self.upper-self.lower)/3.0/self.learn_rate)
+        x0 = asarray(x0)
+        xc = squeeze(random.normal(0, 1.0, size=self.dims))
+
+        xnew = x0 + xc*std*self.learn_rate
+        return xnew
+
+    def update_temp(self):
+        self.k += 1
+        self.T = self.T0 / log(self.k+1.0)
+        return
+
+class _state(object):
+    def __init__(self):
+        self.x = None
+        self.cost = None
+
+# Parallel simulated annealing
+
+def panneal(func, x0, args=(), schedule='fast', full_output=0,
+           T0=None, Tf=1e-12, maxeval=None, maxaccept=None, maxiter=400,
+           boltzmann=1.0, learn_rate=0.5, feps=1e-6, quench=1.0, m=1.0, n=1.0,
+           lower=-100, upper=100, dwell=50, disp=True, cores=2, interv=20):
+    """Minimize a function using a parallel simulated annealing.
+
+    Schedule is a schedule class implementing the annealing schedule.
+    Available ones are 'fast', 'cauchy', 'boltzmann'
+
+    Parameters
+    ----------
+    func : callable f(x, *args)
+        Function to be optimized.
+    x0 : ndarray
+        Initial guess.
+    args : tuple
+        Extra parameters to `func`.
+    schedule : base_schedule
+        Annealing schedule to use (a class).
+    full_output : bool
+        Whether to return optional outputs.
+    T0 : float
+        Initial Temperature (estimated as 1.2 times the largest
+        cost-function deviation over random points in the range).
+    Tf : float
+        Final goal temperature.
+    maxeval : int
+        Maximum function evaluations.
+    maxaccept : int
+        Maximum changes to accept.
+    maxiter : int
+        Maximum cooling iterations.
+    learn_rate : float
+        Scale constant for adjusting guesses.
+    boltzmann : float
+        Boltzmann constant in acceptance test
+        (increase for less stringent test at each temperature).
+    feps : float
+        Stopping relative error tolerance for the function value in
+        last four coolings.
+    quench, m, n : float
+        Parameters to alter fast_sa schedule.
+    lower, upper : float or ndarray
+        Lower and upper bounds on `x`.
+    dwell : int
+        The number of times to search the space at each temperature.
+    disp : bool
+        Set to True to print convergence messages.
+    cores : number of cores (nodes) on the network
+    interv : nomber of iterations before information exchange
+
+    Returns
+    -------
+    xmin : ndarray
+        Point giving smallest value found.
+    Jmin : float
+        Minimum value of function found.
+    T : float
+        Final temperature.
+    feval : int
+        Number of function evaluations.
+    iters : int
+        Number of cooling iterations.
+    accept : int
+        Number of tests accepted.
+    retval : int
+        Flag indicating stopping condition::
+
+                0 : Points no longer changing
+                1 : Cooled to final temperature
+                2 : Maximum function evaluations
+                3 : Maximum cooling iterations reached
+                4 : Maximum accepted query locations reached
+                5 : Final point not the minimum amongst encountered points
+
+    Notes
+    -----
+    Simulated annealing is a random algorithm which uses no derivative
+    information from the function being optimized. In practice it has
+    been more useful in discrete optimization than continuous
+    optimization, as there are usually better algorithms for continuous
+    optimization problems.
+
+    Some experimentation by trying the difference temperature
+    schedules and altering their parameters is likely required to
+    obtain good performance.
+
+    The randomness in the algorithm comes from random sampling in numpy.
+    To obtain the same results you can call numpy.random.seed with the
+    same seed immediately before calling scipy.optimize.anneal.
+
+    We give a brief description of how the three temperature schedules
+    generate new points and vary their temperature. Temperatures are
+    only updated with iterations in the outer loop. The inner loop is
+    over loop over xrange(dwell), and new points are generated for
+    every iteration in the inner loop. (Though whether the proposed
+    new points are accepted is probabilistic.)
+
+    For readability, let d denote the dimension of the inputs to func.
+    Also, let x_old denote the previous state, and k denote the
+    iteration number of the outer loop. All other variables not
+    defined below are input variables to scipy.optimize.anneal itself.
+
+    In the 'fast' schedule the updates are ::
+
+        u ~ Uniform(0, 1, size=d)
+        y = sgn(u - 0.5) * T * ((1+ 1/T)**abs(2u-1) -1.0)
+        xc = y * (upper - lower)
+        x_new = x_old + xc
+
+        c = n * exp(-n * quench)
+        T_new = T0 * exp(-c * k**quench)
+
+
+    In the 'cauchy' schedule the updates are ::
+
+        u ~ Uniform(-pi/2, pi/2, size=d)
+        xc = learn_rate * T * tan(u)
+        x_new = x_old + xc
+
+        T_new = T0 / (1+k)
+
+    In the 'boltzmann' schedule the updates are ::
+
+        std = minimum( sqrt(T) * ones(d), (upper-lower) / (3*learn_rate) )
+        y ~ Normal(0, std, size=d)
+        x_new = x_old + learn_rate * y
+
+        T_new = T0 / log(1+k)
+
+    """
+
+    opts = {'schedule'  : schedule,
+            'T0'        : T0,
+            'Tf'        : Tf,
+            'maxfev'    : maxeval,
+            'maxaccept' : maxaccept,
+            'maxiter'   : maxiter,
+            'boltzmann' : boltzmann,
+            'learn_rate': learn_rate,
+            'ftol'      : feps,
+            'quench'    : quench,
+            'm'         : m,
+            'n'         : n,
+            'lower'     : lower,
+            'upper'     : upper,
+            'dwell'     : dwell,
+            'disp'      : disp,
+            'interv'    : interv,
+            'cores'     : cores}
+
+    # call _minimize_anneal full_output=True in order to always retrieve
+    # retval (aka info['status'])
+    
+    x, info = _cluster_algorithm(func, x0, args, opts, full_output)
+    
+    if full_output:
+        return x, info['fun'], info['T'], info['nfev'], info['nit'], \
+            info['accept'], info['status']
+    else:
+        return x, info['status']
+
+def _cluster_algorithm(func, x0, args, options, full_output=True):
+    """ Minimization of scalar function of one or more variables using the
+    cluster algorithm (parallel simulated annealing).
+    Based on the article of _Parallel Simulated Annealing Algorithms_
+        of D. JANAKI RAM,1 T. H. SREENIVAS, AND K. GANAPATHY SUBRAMANIAM
+        JOURNAL OF PARALLEL AND DISTRIBUTED COMPUTING 37, 207-212 (1996)
+
+    Options for the simulated annealing algorithm are:
+        disp : bool
+            Set to True to print convergence messages.
+        schedule : str
+            Annealing schedule to use. One of: 'fast', 'cauchy' or
+            'boltzmann'.
+        T0 : float
+            Initial Temperature (estimated as 1.2 times the largest
+            cost-function deviation over random points in the range).
+        Tf : float
+            Final goal temperature.
+        maxfev : int
+            Maximum number of function evaluations to make.
+        maxaccept : int
+            Maximum changes to accept.
+        maxiter : int
+            Maximum number of iterations to perform.
+        boltzmann : float
+            Boltzmann constant in acceptance test (increase for less
+            stringent test at each temperature).
+        learn_rate : float
+            Scale constant for adjusting guesses.
+        ftol : float
+            Relative error in ``fun(x)`` acceptable for convergence.
+        quench, m, n : float
+            Parameters to alter fast_sa schedule.
+        lower, upper : float or ndarray
+            Lower and upper bounds on `x`.
+        dwell : int
+            The number of times to search the space at each temperature.
+
+    This function is called by the `minimize` function with
+    `method=anneal`. It is not supposed to be called directly.
+    """
+    
+    schedule   = options.get('schedule', 'fast')
+    T0         = options.get('T0')
+    Tf         = options.get('Tf', 1e-12)
+    maxeval    = options.get('maxfev')
+    maxaccept  = options.get('maxaccept')
+    maxiter    = options.get('maxiter', 400)
+    boltzmann  = options.get('boltzmann', 1.0)
+    learn_rate = options.get('learn_rate', 0.5)
+    feps       = options.get('ftol', 1e-6)
+    quench     = options.get('quench', 1.0)
+    m          = options.get('m', 1.0)
+    n          = options.get('n', 1.0)
+    lower      = options.get('lower', -100)
+    upper      = options.get('upper', 100)
+    dwell      = options.get('dwell', 50)
+    disp       = options.get('disp', False)
+    interv     = options.get('interv', 50)
+    cores      = options.get('cores', 2)
+    
+    lower = asarray(lower)
+    upper = asarray(upper)
+    
+    schedule = eval(schedule+'_sa()')
+    # initialize the schedule
+    schedule.init(dims=shape(x0),func=func,args=args,boltzmann=boltzmann,T0=T0,
+                  learn_rate=learn_rate, lower=lower, upper=upper,
+                  m=m, n=n, quench=quench, dwell=dwell)
+                  
+    state = _state()
+    if x0 != None:
+        X0 = [asarray(x0)]*cores
+    else:
+        X0 = [schedule.getstart_temp(state)]*cores
+    if T0!=None:
+        schedule.T0 = T0
+    elif x0!=None:
+        schedule.getstart_temp(state)
+    schedule.T = schedule.T0
+    
+    best_state = _state()
+    best_state.cost = numpy.Inf
+    last_iter = 0
+    fqueue = [100, 300, 500, 700]
+    manager = Manager()
+    
+    while 1:
+        manager_results = [manager.dict() for i in range(cores)]
+        list_anneal = [_minimize_anneal(func, X0[i], args, dwell, copy(schedule), manager_results[i],
+                        interv, 'thread_{}'.format(i)) for i in range(cores)]
+        for p in list_anneal:
+            p.start()
+                    
+        # Wait for processes to stop
+        for p in list_anneal:
+            p.join()
+
+        last_state = _state()
+        last_state.cost = numpy.Inf
+        
+        # for p in list_anneal:
+        for i in range(len(list_anneal)):
+            p = list_anneal[i]
+            if p.result['best_state'].cost < last_state.cost:
+                last_state = p.result['best_state']
+                schedule = p.result['schedule']
+                if p.result['best_state'].cost < best_state.cost:
+                    best_state = p.result['best_state']
+
+        if schedule.iters - last_iter > 0:
+            fqueue.append(last_state.cost)
+            fqueue.pop(0)
+            last_iter = schedule.iters
+        
+        # Stopping conditions
+        # 0) last saved values of f from each cooling step
+        #     are all very similar (effectively cooled)
+        # 1) Tf is set and we are below it
+        # 2) maxeval is set and we are past it
+        # 3) maxiter is set and we are past it
+        # 4) maxaccept is set and we are past it
+        
+        af = asarray(fqueue)*1.0
+        if all(abs((af-af[0])/af[0]) < feps):
+            retval = 0
+            if abs(af[-1]-best_state.cost) > feps*10:
+                retval = 5
+                if disp:
+                    print "Warning: Cooled to %f at %s but this is not" \
+                          % (squeeze(last_state.cost),
+                             str(squeeze(last_state.x))) \
+                          + " the smallest point found."
+            break
+        if (Tf is not None) and (schedule.T < Tf):
+            retval = 1
+            break
+        if (maxeval is not None) and (schedule.feval > maxeval):
+            retval = 2
+            break
+        if (schedule.iters > maxiter):
+            if disp:
+                print "Warning: Maximum number of iterations exceeded."
+            retval = 3
+            break
+        if (maxaccept is not None) and (schedule.accepted > maxaccept):
+            retval = 4
+            break
+
+        else:
+            # Stopping conditions not reached
+            X0 = [last_state.x.copy()]
+            for i in range(cores-1):
+                X0.append(schedule.update_guess(X0[0]))
+                
+    # OUTPUTS:
+    info = {'solution': best_state.x,
+            'fun'     : best_state.cost,
+            'T'       : schedule.T,
+            'nfev'    : schedule.feval,
+            'nit'     : schedule.iters,
+            'accept'  : schedule.accepted,
+            'status'  : retval,
+            'success' : retval <= 1}
+    info['message'] = {0: 'Points no longer changing',
+                       1: 'Cooled to final temperature',
+                       2: 'Maximum function evaluations',
+                       3: 'Maximum cooling iterations reached',
+                       4: 'Maximum accepted query locations reached',
+                       5: 'Final point not the minimum amongst '
+                          'encountered points',
+                       6: 'Parallel annealing: no end reached '}[retval]
+
+    return best_state.x, info
+
+
+class _minimize_anneal(Thread):
+# class _minimize_anneal(Process):
+    """
+    Minimization of scalar function of one or more variables using the
+    simulated annealing algorithm.
+    Inheritance from the processing.Process object
+
+    Options for the simulated annealing algorithm are:
+        schedule : str
+            Annealing schedule to use. One of: 'fast', 'cauchy' or
+            'boltzmann'.
+        T0 : float
+            Initial Temperature (estimated as 1.2 times the largest
+            cost-function deviation over random points in the range).
+        Tf : float
+            Final goal temperature.
+        dwell : int
+            The number of times to search the space at each temperature.
+
+    This function is called by the parallel algorithms.
+    It is not supposed to be called directly.
+    """
+
+
+    def __init__(self, func, x0, args, dwell, schedule, result, interv, name):
+        # Process.__init__(self, name=name)
+        Thread.__init__(self, name=name)
+        self.func = func
+        self.x0 = x0
+        self.args = args
+        self.dwell = dwell
+        self.schedule = schedule
+        self.interv = interv
+        self.result = result
+
+    def run(self):
+        func = self.func
+        x0 = self.x0
+        args = self.args
+        schedule = self.schedule
+
+        current_state, last_state, best_state = _state(), _state(), _state()
+
+        last_state.x = asarray(x0).copy()
+        fval = func(x0,*args)
+        schedule.feval += 1
+        last_state.cost = fval
+        best_state.cost = numpy.Inf
+        if last_state.cost < best_state.cost:
+            best_state.cost = fval
+            best_state.x = asarray(x0).copy()
+        
+        for _ in range(self.interv):
+            if schedule.iters*self.dwell < schedule.feval:
+                schedule.update_temp()
+                schedule.iters += 1
+            
+            current_state.x = schedule.update_guess(last_state.x)
+            current_state.cost = func(current_state.x,*args)
+            schedule.feval += 1
+
+            dE = current_state.cost - last_state.cost
+            if schedule.accept_test(dE):
+                last_state.x = current_state.x.copy()
+                last_state.cost = current_state.cost
+                if last_state.cost < best_state.cost:
+                    best_state.x = last_state.x.copy()
+                    best_state.cost = last_state.cost
+
+        self.result['best_state'] = best_state # TODO --just pour test rapide
+        self.result['schedule'] = schedule
+        
+
+if __name__ == "__main__":
+    from numpy import cos
+    # minimum expected at ~-0.195
+    func = lambda x: cos(14.5*x-0.3) + (x+0.2)*x
+    print panneal(func,1.0,full_output=1,upper=3.0,lower=-3.0,feps=1e-4,maxiter=2000,schedule='cauchy')
+    print panneal(func,1.0,full_output=1,upper=3.0,lower=-3.0,feps=1e-4,maxiter=2000,schedule='fast')
+    print panneal(func,1.0,full_output=1,upper=3.0,lower=-3.0,feps=1e-4,maxiter=2000,schedule='boltzmann')
+
+    # minimum expected at ~[-0.195, -0.1]
+    func = lambda x: cos(14.5*x[0]-0.3) + (x[1]+0.2)*x[1] + (x[0]+0.2)*x[0]
+    print panneal(func,[1.0, 1.0],full_output=1,upper=[3.0, 3.0],lower=[-3.0, -3.0],feps=1e-4,maxiter=2000,schedule='cauchy')
+    print panneal(func,[1.0, 1.0],full_output=1,upper=[3.0, 3.0],lower=[-3.0, -3.0],feps=1e-4,maxiter=2000,schedule='fast')
+    print panneal(func,[1.0, 1.0],full_output=1,upper=[3.0, 3.0],lower=[-3.0, -3.0],feps=1e-4,maxiter=2000,schedule='boltzmann')
Common subdirectories: optimize_old//tests and optimize_para//tests
